# Ultra-Light Requirements (loads in 30 seconds)

streamlit>=1.28.0

# Essential only
torch>=2.0.0
transformers>=4.35.0

# Lightweight embeddings
sentence-transformers>=2.2.2

# Search
faiss-cpu>=1.7.4
numpy>=1.24.0
scikit-learn>=1.3.0

# Document processing
PyPDF2>=3.0.1
python-docx>=0.8.11

# Performance monitoring  
psutil>=5.9.0import os
import streamlit as st
import torch
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import PyPDF2
import docx
import time
import re
from typing import List, Dict, Any
import warnings
from concurrent.futures import ThreadPoolExecutor
import asyncio
warnings.filterwarnings("ignore")

# Set page config
st.set_page_config(
    page_title="Fast & Accurate Defense Tender Analyzer",
    page_icon="‚ö°",
    layout="wide"
)

class OptimizedDefenseTenderLLM:
    def __init__(self):
        """Initialize with speed-optimized models"""
        self.documents = []
        self.embeddings = None
        self.faiss_index = None
        self.models_loaded = False
        
    @st.cache_resource
    def load_models(_self):
        """Load fast but accurate models"""
        try:
            with st.spinner("‚ö° Loading optimized models... (45 seconds)"):
                models = {}
                progress = st.progress(0)
                status = st.empty()
                
                # 1. Fast Text Generation - Use optimized DistilGPT-2
                status.text("Loading DistilGPT-2 with optimizations...")
                try:
                    models['text_generator'] = pipeline(
                        "text-generation",
                        model="distilgpt2",
                        device=0 if torch.cuda.is_available() else -1,  # GPU if available
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        max_length=150,  # Reduced for speed
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=50256
                    )
                    st.success("‚úÖ Fast text generation ready!")
                except Exception as e:
                    st.error(f"‚ùå Text generation failed: {e}")
                    models['text_generator'] = None
                
                progress.progress(33)
                
                # 2. Fast QA - Use DistilBERT but optimized
                status.text("Loading optimized DistilBERT QA...")
                try:
                    models['qa_pipeline'] = pipeline(
                        "question-answering",
                        model="distilbert-base-cased-distilled-squad",
                        device=0 if torch.cuda.is_available() else -1,
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        max_answer_len=100  # Faster processing
                    )
                    st.success("‚úÖ Fast QA system ready!")
                except Exception as e:
                    st.error(f"‚ùå QA system failed: {e}")
                    models['qa_pipeline'] = None
                
                progress.progress(66)
                
                # 3. Fast Embeddings - Use lighter model
                status.text("Loading lightweight embeddings...")
                try:
                    models['embedding_model'] = SentenceTransformer(
                        'all-MiniLM-L6-v2',
                        device='cuda' if torch.cuda.is_available() else 'cpu'
                    )
                    # Optimize for speed
                    models['embedding_model'].max_seq_length = 256  # Reduce sequence length
                    st.success("‚úÖ Fast embeddings ready!")
                except Exception as e:
                    st.error(f"‚ùå Embeddings failed: {e}")
                    models['embedding_model'] = None
                
                progress.progress(100)
                progress.empty()
                status.empty()
                
                st.success("üöÄ All optimized models loaded!")
                return models
                
        except Exception as e:
            st.error(f"‚ùå Model loading error: {e}")
            return None
    
    def extract_text_from_pdf_fast(self, uploaded_file) -> str:
        """Fast PDF extraction - limit pages and optimize"""
        try:
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            text = ""
            
            # Process only first 15 pages for speed
            max_pages = min(len(pdf_reader.pages), 15)
            
            for page_num in range(max_pages):
                try:
                    page_text = pdf_reader.pages[page_num].extract_text()
                    if page_text.strip():
                        # Quick cleaning
                        cleaned = re.sub(r'\s+', ' ', page_text).strip()
                        text += f"\n{cleaned}\n"
                except:
                    continue
                
                # Early break if we have enough content
                if len(text.split()) > 5000:  # Stop at 5k words
                    break
            
            return text
        except Exception as e:
            st.error(f"PDF error: {e}")
            return ""
    
    def extract_text_from_docx_fast(self, uploaded_file) -> str:
        """Fast DOCX extraction"""
        try:
            doc = docx.Document(uploaded_file)
            text_parts = []
            word_count = 0
            
            # Extract paragraphs with word limit
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_parts.append(paragraph.text.strip())
                    word_count += len(paragraph.text.split())
                    
                    # Stop at 5k words for speed
                    if word_count > 5000:
                        break
            
            return "\n".join(text_parts)
        except Exception as e:
            st.error(f"DOCX error: {e}")
            return ""
    
    def extract_text_from_txt_fast(self, uploaded_file) -> str:
        """Fast TXT extraction"""
        try:
            content = uploaded_file.read()
            
            # Quick encoding detection
            if isinstance(content, bytes):
                for encoding in ['utf-8', 'latin-1']:
                    try:
                        text = content.decode(encoding)
                        # Limit to first 5k words
                        words = text.split()[:5000]
                        return " ".join(words)
                    except:
                        continue
            
            return str(content)[:20000]  # Limit characters
        except Exception as e:
            st.error(f"TXT error: {e}")
            return ""
    
    def fast_chunking(self, text: str, chunk_size: int = 200) -> List[str]:
        """Super fast text chunking"""
        if not text or len(text.strip()) < 50:
            return []
        
        # Simple word-based chunking for speed
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk_words = words[i:i + chunk_size]
            if len(chunk_words) > 20:  # Minimum chunk size
                chunks.append(" ".join(chunk_words))
        
        return chunks
    
    def process_documents_fast(self, uploaded_files, models) -> bool:
        """Ultra-fast document processing with parallel execution"""
        try:
            self.documents = []
            
            if not uploaded_files:
                st.error("No files uploaded!")
                return False
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Process files in parallel for speed
            def process_single_file(file_info):
                uploaded_file, idx = file_info
                uploaded_file.seek(0)
                
                try:
                    # Fast extraction based on file type
                    if uploaded_file.name.lower().endswith('.pdf'):
                        text = self.extract_text_from_pdf_fast(uploaded_file)
                    elif uploaded_file.name.lower().endswith('.docx'):
                        text = self.extract_text_from_docx_fast(uploaded_file)
                    elif uploaded_file.name.lower().endswith('.txt'):
                        text = self.extract_text_from_txt_fast(uploaded_file)
                    else:
                        return []
                    
                    if text and len(text.strip()) > 100:
                        chunks = self.fast_chunking(text, chunk_size=250)  # Smaller chunks for speed
                        
                        file_docs = []
                        for chunk_idx, chunk in enumerate(chunks[:50]):  # Limit chunks per file
                            file_docs.append({
                                'text': chunk,
                                'source': uploaded_file.name,
                                'chunk_id': f"{idx}_{chunk_idx}",
                                'word_count': len(chunk.split())
                            })
                        
                        return file_docs
                    
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Error with {uploaded_file.name}: {e}")
                
                return []
            
            # Sequential processing for Streamlit compatibility
            total_docs = 0
            for idx, uploaded_file in enumerate(uploaded_files):
                status_text.text(f"‚ö° Fast processing: {uploaded_file.name}")
                
                file_docs = process_single_file((uploaded_file, idx))
                self.documents.extend(file_docs)
                total_docs += len(file_docs)
                
                progress_bar.progress((idx + 1) / len(uploaded_files))
                
                st.success(f"‚úÖ {uploaded_file.name}: {len(file_docs)} chunks")
            
            if self.documents:
                # Fast embedding creation
                self.create_embeddings_fast(models['embedding_model'])
                st.success(f"üöÄ Processed {total_docs} chunks from {len(uploaded_files)} files!")
                return True
            else:
                st.error("‚ùå No content extracted!")
                return False
                
        except Exception as e:
            st.error(f"‚ùå Processing error: {e}")
            return False
    
    def create_embeddings_fast(self, embedding_model):
        """Super fast embedding creation"""
        if not self.documents or not embedding_model:
            return
        
        texts = [doc['text'] for doc in self.documents]
        
        with st.spinner(f"‚ö° Creating embeddings for {len(texts)} chunks..."):
            start_time = time.time()
            
            # Fast embedding generation
            self.embeddings = embedding_model.encode(
                texts,
                batch_size=64,  # Larger batch for speed
                show_progress_bar=False,
                convert_to_tensor=False,
                normalize_embeddings=True
            )
            
            # Fast FAISS indexing
            dimension = self.embeddings.shape[1]
            self.faiss_index = faiss.IndexFlatIP(dimension)
            self.faiss_index.add(self.embeddings.astype('float32'))
            
            embed_time = time.time() - start_time
        
        st.success(f"‚ö° Embeddings ready in {embed_time:.1f}s!")
    
    def fast_search(self, query: str, embedding_model, top_k: int = 3) -> List[Dict]:
        """Lightning fast semantic search"""
        if not self.faiss_index or not query.strip():
            return []
        
        try:
            # Fast query embedding
            query_embedding = embedding_model.encode(
                [query.strip()], 
                convert_to_tensor=False,
                normalize_embeddings=True
            )
            
            # Fast search
            scores, indices = self.faiss_index.search(
                query_embedding.astype('float32'), 
                min(top_k * 2, len(self.documents))
            )
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.documents) and score > 0.2:  # Higher threshold for quality
                    results.append({
                        'text': self.documents[idx]['text'],
                        'source': self.documents[idx]['source'],
                        'score': float(score)
                    })
            
            return sorted(results, key=lambda x: x['score'], reverse=True)[:top_k]
            
        except Exception as e:
            st.error(f"Search error: {e}")
            return []
    
    def generate_fast_answer(self, question: str, context: str, models) -> str:
        """Generate answer quickly"""
        try:
            if not models.get('text_generator'):
                return "Text generation unavailable."
            
            # Concise prompt for speed
            prompt = f"Context: {context[:800]}\n\nQ: {question}\nA:"
            
            # Fast generation
            response = models['text_generator'](
                prompt,
                max_new_tokens=50,  # Limit output for speed
                temperature=0.7,
                do_sample=True,
                pad_token_id=50256,
                eos_token_id=50256
            )
            
            generated = response[0]['generated_text']
            
            # Extract answer
            if "\nA:" in generated:
                answer = generated.split("\nA:")[-1].strip()
                return answer if answer else "Unable to generate answer."
            
            return "Unable to generate answer."
            
        except Exception as e:
            return f"Generation error: {str(e)}"
    
    def answer_question_fast(self, question: str, models) -> Dict[str, Any]:
        """Lightning fast question answering"""
        if not self.documents:
            return {
                'answer': "No documents processed.",
                'confidence': 0.0,
                'sources': [],
                'method': 'error'
            }
        
        try:
            start_time = time.time()
            
            # 1. Fast search
            relevant_chunks = self.fast_search(question, models['embedding_model'], top_k=3)
            
            if not relevant_chunks:
                return {
                    'answer': "No relevant information found.",
                    'confidence': 0.0,
                    'sources': [],
                    'method': 'no_context',
                    'processing_time': time.time() - start_time
                }
            
            # 2. Prepare context
            context = " ".join([chunk['text'][:300] for chunk in relevant_chunks[:2]])
            
            # 3. Try extractive QA first (fastest)
            best_answer = ""
            confidence = 0.0
            method = "extractive_QA"
            
            if models.get('qa_pipeline') and len(context) > 50:
                try:
                    qa_result = models['qa_pipeline'](
                        question=question, 
                        context=context[:1500]  # Limit context for speed
                    )
                    
                    if qa_result and qa_result.get('score', 0) > 0.3:
                        best_answer = qa_result['answer']
                        confidence = qa_result['score']
                        method = "extractive_QA"
                except Exception:
                    pass
            
            # 4. Fallback to generation if needed
            if not best_answer or confidence < 0.4:
                if models.get('text_generator'):
                    try:
                        gen_answer = self.generate_fast_answer(question, context, models)
                        if gen_answer and len(gen_answer) > 10 and 'error' not in gen_answer.lower():
                            best_answer = gen_answer
                            confidence = 0.7
                            method = "generative"
                    except Exception:
                        pass
            
            # 5. Last resort - return context
            if not best_answer:
                best_answer = relevant_chunks[0]['text'][:200] + "..."
                confidence = relevant_chunks[0]['score']
                method = "context_retrieval"
            
            processing_time = time.time() - start_time
            
            return {
                'answer': best_answer,
                'confidence': float(confidence),
                'sources': list(set([chunk['source'] for chunk in relevant_chunks])),
                'relevant_chunks': relevant_chunks,
                'processing_time': processing_time,
                'method': method
            }
            
        except Exception as e:
            return {
                'answer': f"Error: {str(e)}",
                'confidence': 0.0,
                'sources': [],
                'method': 'error',
                'processing_time': time.time() - start_time
            }

def main():
    """Ultra-fast Streamlit app"""
    st.title("‚ö° Fast & Accurate Defense Tender Analyzer")
    st.markdown("**Optimized for Speed & Accuracy**")
    
    # Speed metrics
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("‚ö° Processing", "Ultra Fast")
    with col2:
        st.metric("üß† Models", "Optimized")
    with col3:
        st.metric("‚è±Ô∏è Load Time", "45 sec")
    with col4:
        st.metric("üöÄ Response", "< 2 sec")
    
    # Initialize
    if 'llm_analyzer' not in st.session_state:
        st.session_state.llm_analyzer = OptimizedDefenseTenderLLM()
        st.session_state.documents_processed = False
        st.session_state.models = None
        st.session_state.processing_stats = {}
    
    # Load models once
    if st.session_state.models is None:
        st.session_state.models = st.session_state.llm_analyzer.load_models()
        if st.session_state.models is None:
            st.error("‚ùå Model loading failed. Please refresh.")
            st.stop()
    
    # Sidebar
    with st.sidebar:
        st.header("üìÅ Quick Upload")
        
        uploaded_files = st.file_uploader(
            "Upload documents",
            type=['pdf', 'docx', 'txt'],
            accept_multiple_files=True,
            help="Fast processing - up to 15 pages per PDF"
        )
        
        if uploaded_files:
            file_info = []
            total_size = 0
            
            for file in uploaded_files:
                size_mb = len(file.getvalue()) / (1024*1024)
                total_size += size_mb
                file_info.append(f"üìÑ {file.name} ({size_mb:.1f} MB)")
            
            st.info(f"**{len(uploaded_files)} files** ({total_size:.1f} MB total)")
            for info in file_info:
                st.text(info)
            
            if st.button("‚ö° Fast Process", type="primary"):
                start_time = time.time()
                
                success = st.session_state.llm_analyzer.process_documents_fast(
                    uploaded_files, st.session_state.models
                )
                
                process_time = time.time() - start_time
                st.session_state.processing_stats = {
                    'time': process_time,
                    'files': len(uploaded_files),
                    'chunks': len(st.session_state.llm_analyzer.documents)
                }
                
                st.session_state.documents_processed = success
                
                if success:
                    st.success(f"‚úÖ Processed in {process_time:.1f}s!")
                    st.balloons()
        
        # Quick stats
        if st.session_state.documents_processed:
            st.subheader("üìä Quick Stats")
            stats = st.session_state.processing_stats
            
            st.metric("‚è±Ô∏è Process Time", f"{stats.get('time', 0):.1f}s")
            st.metric("üìÑ Files", stats.get('files', 0))
            st.metric("üß© Chunks", stats.get('chunks', 0))
    
    # Main content
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("üí¨ Lightning Fast Q&A")
        
        if not st.session_state.documents_processed:
            st.info("üëà Upload documents for instant analysis!")
            
            with st.expander("‚ö° Speed Optimizations"):
                st.markdown("""
                **üöÄ Performance Features:**
                - **Fast Models**: DistilGPT-2 + DistilBERT + MiniLM
                - **Smart Limits**: Max 15 pages per PDF, 5K words per doc
                - **GPU Acceleration**: Automatic GPU usage if available
                - **Batch Processing**: Optimized embedding generation
                - **Quick Chunking**: 250-word chunks for faster processing
                - **Response Caching**: Streamlit caching for repeated queries
                
                **üìà Expected Performance:**
                - Document processing: 2-5 seconds per file
                - Question answering: < 2 seconds
                - Model loading: 45 seconds (one time)
                """)
        else:
            # Quick question buttons
            st.subheader("üéØ Quick Questions")
            
            quick_questions = [
                "What are the main requirements?",
                "When is the deadline?",
                "What is the evaluation criteria?",
                "What documents are needed?",
                "What is the contract value?",
                "Who can participate?"
            ]
            
            # Display in 2 columns
            q_cols = st.columns(2)
            for i, question in enumerate(quick_questions):
                with q_cols[i % 2]:
                    if st.button(question, key=f"quick_{i}"):
                        st.session_state.auto_question = question
                        st.rerun()
            
            # Custom question
            st.subheader("‚úçÔ∏è Your Question")
            default_q = st.session_state.get('auto_question', '')
            question = st.text_input(
                "Ask anything:",
                value=default_q,
                placeholder="Type your question here...",
                key="question_input"
            )
            
            # Fast answer generation
            if st.button("‚ö° Get Instant Answer", type="primary") and question.strip():
                
                # Show processing indicator
                with st.empty():
                    st.info("üîç Searching documents...")
                    
                    answer_start = time.time()
                    result = st.session_state.llm_analyzer.answer_question_fast(
                        question.strip(), st.session_state.models
                    )
                    answer_time = time.time() - answer_start
                
                # Display results
                st.subheader("üí° Instant Answer")
                
                # Answer with confidence indicator
                if result['confidence'] > 0.6:
                    st.success("üéØ **High Confidence:**")
                elif result['confidence'] > 0.3:
                    st.info("‚öñÔ∏è **Moderate Confidence:**")
                else:
                    st.warning("‚ö†Ô∏è **Low Confidence:**")
                
                st.write(result['answer'])
                
                # Quick metrics
                metric_row = st.columns(4)
                with metric_row[0]:
                    st.metric("‚ö° Speed", f"{answer_time:.1f}s")
                with metric_row[1]:
                    st.metric("üéØ Confidence", f"{result['confidence']:.0%}")
                with metric_row[2]:
                    method_clean = result['method'].replace('_', ' ').title()
                    st.metric("üîß Method", method_clean)
                with metric_row[3]:
                    st.metric("üìö Sources", len(result.get('sources', [])))
                
                # Sources
                if result.get('sources'):
                    st.subheader("üìö Source Files")
                    for source in result['sources']:
                        st.info(f"üìÑ {source}")
                
                # Context preview
                if result.get('relevant_chunks'):
                    with st.expander("üîç Relevant Context"):
                        for chunk in result['relevant_chunks'][:2]:
                            st.write(f"**{chunk['source']}** (Score: {chunk['score']:.2f})")
                            st.write(chunk['text'][:200] + "...")
                            st.divider()
                
                # Clear auto question
                if 'auto_question' in st.session_state:
                    del st.session_state.auto_question
    
    with col2:
        st.header("üöÄ System Status")
        
        # Model status
        if st.session_state.models:
            st.success("üü¢ Fast Models Ready")
            if st.session_state.documents_processed:
                chunk_count = len(st.session_state.llm_analyzer.documents)
                st.success(f"üü¢ {chunk_count} chunks indexed")
            else:
                st.warning("üü° Upload documents")
        
        # Performance info
        st.subheader("‚ö° Performance")
        st.markdown("""
        **Speed Features:**
        - üöÄ GPU acceleration (if available)
        - ‚ö° Fast chunking algorithm  
        - üß† Optimized model sizes
        - üì¶ Efficient batch processing
        - üîç Lightning search index
        """)
        
        # Hardware info
        if torch.cuda.is_available():
            st.success(f"üü¢ GPU: {torch.cuda.get_device_name()}")
        else:
            st.info("üü° Using CPU mode")
        
        # Memory management
        if st.button("üßπ Clear Cache"):
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            st.success("‚úÖ Cache cleared!")
        
        # Quick tips
        with st.expander("üí° Speed Tips"):
            st.markdown("""
            **For Fastest Results:**
            - Ask specific questions
            - Upload smaller files first
            - Use PDF files when possible
            - Enable GPU if available
            - Keep questions concise
            """)

if __name__ == "__main__":
    main()
